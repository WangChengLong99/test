{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成金庸文本的原始列表文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=pd.read_csv('data/文本.txt',encoding='gbk',sep='aaa',names=['txt'])\n",
    "def head(char):\n",
    "    return char[:1]\n",
    "def mid(char):\n",
    "    return char.find('回')\n",
    "raw['head']=raw.txt.apply(head)\n",
    "raw['mid']=raw.txt.apply(mid)\n",
    "raw['len']=raw.txt.apply(len)\n",
    "chapnum=0\n",
    "for i in range(len(raw.txt)):\n",
    "    if raw['head'][i]=='第' and raw['mid'][i]>0 and raw['len'][i]<30:\n",
    "        chapnum+=1\n",
    "    if raw['txt'][i]==\"附录一:成吉思汗家族\":\n",
    "        chapnum=0\n",
    "    raw.loc[i,'chap']=chapnum\n",
    "del raw['len']\n",
    "del raw['head']\n",
    "del raw['mid']\n",
    "rg=raw.groupby('chap')\n",
    "rg=rg.apply(sum)\n",
    "text_jy=list(rg.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成app描述的原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#训练集\n",
    "file_train=open('data//train.json','rb')\n",
    "tmp = []\n",
    "for line in file_train:\n",
    "    tmp.append(json.loads(line))\n",
    "label_des=[i['label_des'] for i in tmp]\n",
    "text_app=[i['sentence'] for i in tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成了一个可以选择文本某一个document或整个document的选择器，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_select(data):\n",
    "    text_num=len(data)\n",
    "    label=range(text_num)\n",
    "    value=[data[i] for i in label]\n",
    "    text_select=widgets.SelectMultiple(options=list(zip(label,value))+[('ALL',data)],value=data,\n",
    "                                       continuous_update=False)\n",
    "    return text_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_select=generate_text_select(text_jy)#最终传递的是tuple类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse as ana\n",
    "import nltk\n",
    "import numpy as np\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义结巴分词的函数，文本通过参数传递，可以传递多个或一个document，当选择所有document时就是传递了原始文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "out_cloud=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "#out_corpus=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "@out_cloud.capture(clear_output=True,wait=True)\n",
    "def fenci(text,library,stopwords,addwords,delwords,fenci_model,fenci_result,font,mask):\n",
    "    text=[i for i in text]#转化为列表类型\n",
    "    if library:\n",
    "        jieba.load_userdict(library)\n",
    "    if addwords:#注意不要continuous updating\n",
    "        jieba.add_word(addwords)#text的形式自定义文本\n",
    "    if delwords:\n",
    "        jieba.del_word(delwords)#以top20的文本为选项,multiple\n",
    "    if stopwords:\n",
    "        ana.set_stop_words(stopwords)\n",
    "    stoplist=list(pd.read_csv(stopwords,names=['w'],sep='aaa',encoding='utf-8',engine='python').w)\n",
    "    if fenci_result=='分词列表':\n",
    "        corpus_raw=[eval('jieba.'+fenci_model) for i in text]\n",
    "        corpus=[[j for j in i if j not in stoplist] for i in corpus_raw]\n",
    "    if fenci_result=='空格连接的字符串':\n",
    "        #corpus=[' '.join([j for j in eval('jieba.'+fenci_model) if j not in stoplist]) for i in text]\n",
    "        corpus_raw=[eval('jieba.'+fenci_model) for i in text]\n",
    "        corpus=[' '.join(j for j in i if j not in stoplist) for i in corpus_raw]\n",
    "    data=[j for i in text for j in eval('jieba.'+fenci_model) if j not in stoplist]\n",
    "    fdist=nltk.FreqDist(data)\n",
    "    ## 设置图云的属性\n",
    "    cloud=wordcloud.WordCloud(font_path=font,max_words=5000,min_font_size=0.1,relative_scaling=0.1,width=600,height=600,mode='RGBA',background_color='white',stopwords=stopwords,mask=imread(mask))\n",
    "    #'C://Windows/Fonts/simhei.ttf'\n",
    "    cloudimage=cloud.fit_words(fdist)\n",
    "    imarray=np.array(imread(mask))\n",
    "    ##获取颜色\n",
    "    bimColors=wordcloud.ImageColorGenerator(imarray)\n",
    "    ##重置词云颜色\n",
    "    cloudimage.recolor(color_func=bimColors)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cloudimage)\n",
    "    plt.show()\n",
    "    return corpus #返回分好词的文本，列表形式，里面元素可能是分词列表，也可能是空格分隔的分好词的文本。\n",
    "\n",
    "#widgets定义\n",
    "import os\n",
    "\n",
    "label=os.listdir('data/词库')\n",
    "path=['data/词库/'+i for i in label]\n",
    "library=widgets.Dropdown(options=list(zip(label,path))+[('','')],value='',description='导入分词词库')\n",
    "\n",
    "label_stop=os.listdir('data/stopword')\n",
    "path_stop=['data/stopword/'+i for i in label_stop]\n",
    "stopwords=widgets.Dropdown(options=list(zip(label_stop,path_stop)),description='导入停用词')\n",
    "\n",
    "addwords=widgets.Text(description='增加词库词语',continuous_update=False)\n",
    "\n",
    "#delwords=widgets.SelectMultiple(options=ana.extract_tags(jieba.lcut(pd.read_csv('data/文本.txt',encoding='gbk',sep='aaa',names=['txt'])))+[''],  value='')\n",
    "delwords=widgets.Text(description='删减词库词语',continuous_update=False)\n",
    "\n",
    "fenci_result=widgets.Dropdown(options=['分词列表','空格连接的字符串'],value='分词列表')\n",
    "\n",
    "label_model=['精确模式','全模式','搜索模式']\n",
    "value_model=['lcut(i)','lcut(i,cut_all=True)','lcut_for_search(i)']\n",
    "fenci_model=widgets.Dropdown(options=list(zip(label_model,value_model)),value='lcut(i)')\n",
    "\n",
    "label_font=os.listdir('C://Windows/Fonts')\n",
    "value_font=['C://Windows/Fonts/'+i for i in label_font]\n",
    "font=widgets.Dropdown(options=list(zip(label_font,value_font)),value='C://Windows/Fonts/simhei.ttf')\n",
    "\n",
    "label_mask=[i for i in os.listdir('data/背景图') if i.endswith('.jpg')]\n",
    "value_mask=['data/背景图/'+i for i in label_mask]\n",
    "mask=widgets.Dropdown(options=list(zip(label_mask,value_mask)),value='data/背景图/太极.jpg')\n",
    "\n",
    "w=widgets.interactive_output(fenci,{'text':text_select,'library':library,'stopwords':stopwords,'addwords':addwords,'delwords':delwords,\n",
    "                                    'fenci_model':fenci_model,'fenci_result':fenci_result,'font':font,'mask':mask})\n",
    "#cloud=widgets.HBox(children=[widgets.VBox(children=[font,mask],layout=widgets.Layout(width='40%')),out_cloud])\n",
    "cloud=widgets.HBox(children=[widgets.VBox(children=[font,mask,text_select,library,stopwords,addwords,delwords,fenci_result,fenci_model],layout=widgets.Layout(width='40%')),out_cloud],layout=widgets.Layout(height='400px'))\n",
    "#corp=widgets.HBox(children=[widgets.VBox(children=[text_select,library,stopwords,addwords,delwords,fenci_result,fenci_model],layout=widgets.Layout(width='40%')),out_corpus])\n",
    "#widgets.VBox([cloud,corp])\n",
    "#cloud\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cloud=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "#out_corpus=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "@out_cloud.capture(clear_output=True,wait=True)\n",
    "def fenci(text,library,stopwords,addwords,delwords,fenci_model,fenci_result,font,mask,suggest_freq_cut,suggest_freq_add):\n",
    "    text=[i for i in text]#转化为列表类型\n",
    "    if library:\n",
    "        jieba.load_userdict(library)\n",
    "    if addwords:#注意不要continuous updating\n",
    "        jieba.add_word(addwords)#text的形式自定义文本\n",
    "    if delwords:\n",
    "        jieba.del_word(delwords)#以top20的文本为选项,multiple\n",
    "    if stopwords:\n",
    "        ana.set_stop_words(stopwords)\n",
    "    stoplist=list(pd.read_csv(stopwords,names=['w'],sep='aaa',encoding='utf-8',engine='python').w)\n",
    "    if suggest_freq_add:\n",
    "        for i in suggest_freq_add:\n",
    "            jieba.suggest_freq(i,True)\n",
    "    if suggest_freq_cut:\n",
    "        for i in suggest_freq_cut:\n",
    "            jieba.suggest_freq(eval(i),True)\n",
    "    if fenci_result=='分词列表':\n",
    "        corpus_raw=[eval('jieba.'+fenci_model) for i in text]\n",
    "        corpus=[[j for j in i if j not in stoplist] for i in corpus_raw]\n",
    "    if fenci_result=='空格连接的字符串':\n",
    "        #corpus=[' '.join([j for j in eval('jieba.'+fenci_model) if j not in stoplist]) for i in text]\n",
    "        corpus_raw=[eval('jieba.'+fenci_model) for i in text]\n",
    "        corpus=[' '.join(j for j in i if j not in stoplist) for i in corpus_raw]\n",
    "    data=[j for i in text for j in eval('jieba.'+fenci_model) if j not in stoplist]\n",
    "    fdist=nltk.FreqDist(data)\n",
    "    ## 设置图云的属性\n",
    "    cloud=wordcloud.WordCloud(font_path=font,max_words=5000,min_font_size=0.1,relative_scaling=0.1,width=600,height=600,mode='RGBA',background_color='white',stopwords=stopwords,mask=imread(mask))\n",
    "    #'C://Windows/Fonts/simhei.ttf'\n",
    "    cloudimage=cloud.fit_words(fdist)\n",
    "    imarray=np.array(imread(mask))\n",
    "    ##获取颜色\n",
    "    bimColors=wordcloud.ImageColorGenerator(imarray)\n",
    "    ##重置词云颜色\n",
    "    cloudimage.recolor(color_func=bimColors)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cloudimage)\n",
    "    plt.show()\n",
    "    return corpus #返回分好词的文本，列表形式，里面元素可能是分词列表，也可能是空格分隔的分好词的文本。\n",
    "\n",
    "#widgets定义\n",
    "import os\n",
    "\n",
    "suggest_freq_cut=widgets.SelectMultiple(options=[],description='拆分某个长词')\n",
    "suggest_freq_add=widgets.SelectMultiple(options=[],description='保留某个长词')\n",
    "\n",
    "label=os.listdir('data/词库')\n",
    "path=['data/词库/'+i for i in label]\n",
    "library=widgets.Dropdown(options=list(zip(label,path))+[('','')],value='',description='导入分词词库')\n",
    "\n",
    "label_stop=os.listdir('data/stopword')\n",
    "path_stop=['data/stopword/'+i for i in label_stop]\n",
    "stopwords=widgets.Dropdown(options=list(zip(label_stop,path_stop)),description='导入停用词')\n",
    "\n",
    "addwords=widgets.Text(description='增加词库词语',continuous_update=False)\n",
    "\n",
    "#delwords=widgets.SelectMultiple(options=ana.extract_tags(jieba.lcut(pd.read_csv('data/文本.txt',encoding='gbk',sep='aaa',names=['txt'])))+[''],\n",
    "#                               value='')\n",
    "delwords=widgets.Text(description='删减词库词语',continuous_update=False)\n",
    "\n",
    "fenci_result=widgets.Dropdown(options=['分词列表','空格连接的字符串'],value='分词列表')\n",
    "\n",
    "label_model=['精确模式','全模式','搜索模式']\n",
    "value_model=['lcut(i)','lcut(i,cut_all=True)','lcut_for_search(i)']\n",
    "fenci_model=widgets.Dropdown(options=list(zip(label_model,value_model)),value='lcut(i)')\n",
    "\n",
    "label_font=os.listdir('data/Fonts')\n",
    "value_font=['data/Fonts/'+i for i in label_font]\n",
    "font=widgets.Dropdown(options=list(zip(label_font,value_font)),value='data/Fonts/simhei.ttf')\n",
    "\n",
    "label_mask=[i for i in os.listdir('data/背景图') if i.endswith('.jpg')]\n",
    "value_mask=['data/背景图/'+i for i in label_mask]\n",
    "mask=widgets.Dropdown(options=list(zip(label_mask,value_mask)),value='data/背景图/太极.jpg')\n",
    "\n",
    "w=widgets.interactive_output(fenci,{'text':text_select,'library':library,'stopwords':stopwords,'addwords':addwords,'delwords':delwords,\n",
    "                                    'fenci_model':fenci_model,'fenci_result':fenci_result,'font':font,'mask':mask,\n",
    "                                    'suggest_freq_add':suggest_freq_add,'suggest_freq_cut':suggest_freq_cut})\n",
    "cloud=widgets.HBox(\n",
    "[widgets.VBox(children=[font,mask,text_select,library,stopwords,addwords,delwords,fenci_result,fenci_model],layout=widgets.Layout(width='40%')),\n",
    "widgets.VBox([widgets.HBox([suggest_freq_add,suggest_freq_cut]),out_cloud])\n",
    "],\n",
    "layout=widgets.Layout(height='600px')\n",
    ")\n",
    "\n",
    "#cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jieba github 介绍](https://github.com/fxsjy/jieba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'d': 3, 'b': 2, 'c': 2, 'a': 1, 'f': 1})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_nltk=widgets.VBox(children=[widgets.HTML(value='<p>NLTK只能识别用空格 作为词条的分隔方式，因此不能直接用于中文文本的分词。<p><p>一般的做法是先用jieba分词，然后转换为空格分隔的连续文本，再转入NLTK框架使用<p>'),\n",
    "                      widgets.Image(value='data/参数截图/nltk分词.png'.encode('utf-8'),format='url',width='auto',height='auto')],\n",
    "                      layout=widgets.Layout(height='400px'))\n",
    "#可以用nltk对分好词的corpus进行词频统计,要把所有document分好的词全部放在一个列表中\n",
    "nltk_corpus=['a','b','c','d','d','d','c','b','f']\n",
    "nltk.FreqDist(nltk_corpus)\n",
    "#tab_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过text随便输入一段文本，查看词性分词结果，以及特定词性的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure,show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import CDSView,BooleanFilter,ColumnDataSource,DataTable,TableColumn\n",
    "from bokeh.layouts import row,column\n",
    "output_notebook()\n",
    "flag_select=widgets.SelectMultiple(options=[],value=())\n",
    "#documents=widgets.Textarea(value='大鱼吃小鱼,小鱼吃虾米',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "out_psg=widgets.Output()\n",
    "@out_psg.capture(clear_output=True,wait=True)\n",
    "def fenci_cixing(documents): \n",
    "    import jieba.posseg as psg\n",
    "    stoplist=list(pd.read_csv('data/stopword/stopwords.txt',names=['w'],sep='aaa',encoding='utf-8',engine='python').w)\n",
    "    term=[j for i in documents for j in psg.lcut(i) if j.word not in stoplist]\n",
    "    data=pd.DataFrame([{'term':i.word,'flag':i.flag} for i in term])\n",
    "    flag_select.options=list(data.flag.unique())\n",
    "    global source_fenci_cixing\n",
    "    source_fenci_cixing=ColumnDataSource(data)\n",
    "    columns=[TableColumn(field=i, title=i) for i in list(data.columns)]\n",
    "    data_table= DataTable(source=source_fenci_cixing,columns=columns,width=150,height=300)\n",
    "    show(data_table)\n",
    "    return [[j for j in psg.lcut(i) if j.word not in stoplist] for i in documents]\n",
    "\n",
    "out_fenci_cixing_table=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "@out_fenci_cixing_table.capture(clear_output=True,wait=True)\n",
    "def add_fenci_cixing_table(change):\n",
    "    columns=[TableColumn(field=i, title=i) for i in list(source_fenci_cixing.data.keys())[1:]]\n",
    "    show(DataTable(source=source_fenci_cixing,columns=columns,width=200,height=300,\n",
    "                   view=CDSView(source=source, filters=[BooleanFilter([i in change['new'] for i in source_fenci_cixing.data['flag']])]))) \n",
    "flag_select.observe(add_fenci_cixing_table,'value')\n",
    "\n",
    "cankao=widgets.HTML(value=\"\"\"\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>标签</th>\n",
    "<th>含义</th>\n",
    "<th>标签</th>\n",
    "<th>含义</th>\n",
    "<th>标签</th>\n",
    "<th>含义</th>\n",
    "<th>标签</th>\n",
    "<th>含义</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>n</td>\n",
    "<td>普通名词</td>\n",
    "<td>f</td>\n",
    "<td>方位名词</td>\n",
    "<td>s</td>\n",
    "<td>处所名词</td>\n",
    "<td>t</td>\n",
    "<td>时间</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>nr</td>\n",
    "<td>人名</td>\n",
    "<td>ns</td>\n",
    "<td>地名</td>\n",
    "<td>nt</td>\n",
    "<td>机构名</td>\n",
    "<td>nw</td>\n",
    "<td>作品名</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>nz</td>\n",
    "<td>其他专名</td>\n",
    "<td>v</td>\n",
    "<td>普通动词</td>\n",
    "<td>vd</td>\n",
    "<td>动副词</td>\n",
    "<td>vn</td>\n",
    "<td>名动词</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>a</td>\n",
    "<td>形容词</td>\n",
    "<td>ad</td>\n",
    "<td>副形词</td>\n",
    "<td>an</td>\n",
    "<td>名形词</td>\n",
    "<td>d</td>\n",
    "<td>副词</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>m</td>\n",
    "<td>数量词</td>\n",
    "<td>q</td>\n",
    "<td>量词</td>\n",
    "<td>r</td>\n",
    "<td>代词</td>\n",
    "<td>p</td>\n",
    "<td>介词</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>c</td>\n",
    "<td>连词</td>\n",
    "<td>u</td>\n",
    "<td>助词</td>\n",
    "<td>xc</td>\n",
    "<td>其他虚词</td>\n",
    "<td>w</td>\n",
    "<td>标点符号</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PER</td>\n",
    "<td>人名</td>\n",
    "<td>LOC</td>\n",
    "<td>地名</td>\n",
    "<td>ORG</td>\n",
    "<td>机构名</td>\n",
    "<td>TIME</td>\n",
    "\n",
    "<td>时间</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\"\"\",layout=widgets.Layout(width='auto'))\n",
    "\n",
    "w_psg=widgets.interactive_output(fenci_cixing,{'documents':text_select})\n",
    "tab_psg=widgets.HBox(children=[widgets.VBox([text_select,cankao],layout=widgets.Layout(width='50%')),\n",
    "                               widgets.VBox([widgets.Label(value='选择某种词性'),flag_select,widgets.HBox([out_psg,out_fenci_cixing_table])])],\n",
    "                     layout=widgets.Layout(height='400px'))\n",
    "#tab_psg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建了一个全局变量 source_fenci_cixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd07562960c4d9aa87db4fb4797ca99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(VBox(children=(Dropdown(index=247, options=(('8514fix.fon', 'C://Windows/Fonts/85…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#创建了一个全局变量 source_fenci_cixing,这个决定了右图表的展示，函数决定了左图的展示\n",
    "tab_fenci=widgets.Tab(children=[cloud,tab_nltk,tab_psg])\n",
    "tab_fenci.set_title(0,'jieba')\n",
    "tab_fenci.set_title(1,'nltk')\n",
    "tab_fenci.set_title(2,'词性标注')\n",
    "tab_fenci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba_suggest_freq_add_list=()\n",
    "jieba_suggest_freq_cut_list=(\"('黄蓉','道')\",)#一个元素要加逗号\n",
    "if jieba_suggest_freq_add_list:\n",
    "    suggest_freq_add.options=jieba_suggest_freq_add_list\n",
    "    suggest_freq_add.value=jieba_suggest_freq_add_list\n",
    "if jieba_suggest_freq_cut_list:\n",
    "    suggest_freq_cut.options=jieba_suggest_freq_cut_list\n",
    "    suggest_freq_cut.value=jieba_suggest_freq_cut_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据前面调整的结果，获取最终分好词的语料\n",
    "corpus=fenci(text=text_select.value,library=library.value,stopwords=stopwords.value,addwords=addwords.value,delwords=delwords.value,font=font.value,\n",
    "            fenci_model=fenci_model.value,fenci_result=fenci_result.value,mask=mask.value,suggest_freq_add=suggest_freq_add.value,\n",
    "            suggest_freq_cut=suggest_freq_cut.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_flag=fenci_cixing(documents=text_select.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型 bags of words bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以考虑词频也可以不考虑（只考虑是否出现），把词排列好，生成字典，id和词，然后判断每一个词在每一个文本下出现的情况。记录数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1090\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1090\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1090\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1090\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1090\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure,show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import DataTable\n",
    "from bokeh.models import ColumnDataSource,TableColumn\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用add_document部件添加corpus,默认已经添加了前面的corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成字典\n",
    "out_gensim=widgets.Output(layout=widgets.Layout(width='auto'),height='500px')\n",
    "@out_gensim.capture(clear_output=True,wait=True)\n",
    "def gensim_bow(top_k_term,add_documents,no_below,no_above,keep_n,keep_tokens):\n",
    "    dct=Dictionary()\n",
    "    if add_documents:\n",
    "        if add_documents[0]:\n",
    "            for i in add_documents:#请设置成SelectMultiple\n",
    "                dct.add_documents(i)    \n",
    "    dct.filter_extremes(no_below=no_below,no_above=no_above,keep_n=keep_n,keep_tokens=None if not keep_tokens else keep_tokens)\n",
    "    print('dimension{}'.format(len(dct.token2id)),\n",
    "        '\\ndocuments数量:{}'.format(dct.num_docs),\n",
    "          '\\nnum_pos:total number of corpus positions(number of processed words:{})'.format(dct.num_pos),\n",
    "         '\\nnum_nnz:total number of non_zeros in the BOW matrix:{}'.format(dct.num_nnz))\n",
    "    #计算前k个最高term-freq的单词\n",
    "    key_item=sorted([(key1,item) for key1,item in dct.dfs.items()],key=lambda x:x[1],reverse=True)\n",
    "    id_word={item:key for key,item in dct.token2id.items()}\n",
    "    data=[[key,id_word[key],item,item/sum([len(i) for i in add_documents])] for key,item in key_item[:top_k_term]]\n",
    "    top_k=pd.DataFrame(data,columns=['id','term','how many document have term','term/document'])\n",
    "    #print(top_k)\n",
    "    global source\n",
    "    source=ColumnDataSource(top_k)\n",
    "    columns=[TableColumn(field=i, title=i) for i in list(top_k.columns)]\n",
    "    data_table= DataTable(source=source,columns=columns,width=400,height=300)\n",
    "    show(data_table)\n",
    "    #计算每个documents的bow矩阵并且转化为标准矩阵，并删掉低频词列\n",
    "    cor_cop=[]\n",
    "    [cor_cop.extend(j) for j in add_documents]    \n",
    "    #documents_dict=[{id1:count for id1,count in dct.doc2bow(i)} for i in cor_cop]\n",
    "    #documents_bow=np.mat(pd.DataFrame(documents_dict).fillna(0).values)\n",
    "    documents_bow=[dct.doc2bow(i) for i in cor_cop]\n",
    "    return dct,documents_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_term=widgets.IntText(min=100000,max=1000000,step=10000,value=200000,continuous_update=False,layout=widgets.Layout(width='auto'))\n",
    "top_k_term=widgets.IntSlider(min=10,max=100,step=5,value=10,continuous_update=False,description='显示前k个term')\n",
    "add_documents=widgets.SelectMultiple(options=[('语料',corpus)],value=(corpus,),continuous_update=False,layout=widgets.Layout(width='auto'))\n",
    "no_below=widgets.IntSlider(description='最低包含term的document数',min=0,max=sum([len(i) for i in add_documents.value]),freq=1,value=0,\n",
    "                           continuous_update=False,layout=widgets.Layout(width='auto'),style = {'description_width': 'initial'})\n",
    "no_above=widgets.FloatSlider(description='最高包含term的document百分比',min=0,max=1,freq=0.05,value=1,\n",
    "                             continuous_update=False,layout=widgets.Layout(width='auto'),style = {'description_width': 'initial'})\n",
    "keep_n=widgets.IntText(description='数据维度，保留前n个word',min=10000,max=2000000,step=1000,value=100000,\n",
    "                       continuous_update=False,layout=widgets.Layout(width='auto'),style = {'description_width': 'initial'})\n",
    "keep_tokens=widgets.Text(continuous_update=False,layout=widgets.Layout(width='auto'),description='无视规则必然保留的term',\n",
    "                        style = {'description_width': 'initial'})\n",
    "vbox_gensim=widgets.VBox(children=[top_k_term,add_documents,no_below,no_above,keep_n,keep_tokens],\n",
    "                         layout=widgets.Layout(width='40%',height='400px'))\n",
    "\n",
    "# 定义更新corpus选项:\n",
    "def update_add_documents(new_corpus,corpus_name):#请用列表将所有要添加的corpus(list in list形式)包括。\n",
    "    options=list(add_documents.options)\n",
    "    for i,name in zip(new_corpus,corpus_name):\n",
    "        options.append((name,i))\n",
    "    add_documents.options=options\n",
    "    add_documents.value=([],)\n",
    "\n",
    "# 根据corpus中documents个数，更新no_below:\n",
    "def update_no_below(change):\n",
    "    if change['new']:\n",
    "        a=no_below.max\n",
    "        no_below.max=a+sum([len(i) for i in change['new']])\n",
    "add_documents.observe(update_no_below,'value')   \n",
    "\n",
    "w_gensim=widgets.interactive_output(gensim_bow,{'top_k_term':top_k_term,\n",
    "                                                'add_documents':add_documents,'no_below':no_below,'no_above':no_above,\n",
    "                                                'keep_n':keep_n,'keep_tokens':keep_tokens})\n",
    "tab_gensim=widgets.HBox([vbox_gensim,out_gensim],layout=widgets.Layout(height='400px'))\n",
    "#tab_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_documents.options\n",
    "#add_documents.value\n",
    "#no_below.max\n",
    "#no_below.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn实现\n",
    "\n",
    "**用数据输入形式为列表，列表元素为代表文章的字符串，一个字符串代表一篇文章，字符串是已经分割好的。**CountVectorizer同样适用于中文;\n",
    "CountVectorizer是通过fit_transform函数将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在第i个文本下的词频。即各个词语出现的次数，通过get_feature_names()可看到所有文本的关键字，通过toarray()可看到词频矩阵的结果。\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count=CountVectorizer()\n",
    "#vector=count.fit_transform(word_list)\n",
    "#vector.todense()#将所有词作为特征，该字符串有则为频数，无则为0,\n",
    "#改为从部件中添加\n",
    "out_sklearn=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "@out_sklearn.capture(clear_output=True,wait=True)\n",
    "def sklearn_bow(corpus_sklearn,analyzer,stop_words,token_pattern,max_df,min_df,max_features,binary,k_j_term):\n",
    "    data=[j for i in corpus_sklearn for j in i]\n",
    "    count=CountVectorizer(analyzer=analyzer,\n",
    "                          #stop_words=eval('['+stop_words+']'),\n",
    "                          #token_pattern=r\"{}\".format(token_pattern),\n",
    "                          max_df=max_df,min_df=min_df,max_features=max_features,binary=binary)\n",
    "    vector=count.fit_transform(data)\n",
    "    term=pd.DataFrame([[i,count.vocabulary_[i]] for i in sorted(count.vocabulary_,reverse=True)],columns=['term','term_total_appear'])\n",
    "    source=ColumnDataSource(term.iloc[k_j_term[0]:k_j_term[1],:])\n",
    "    columns=[TableColumn(field=i,title=i) for i in list(term.iloc[:,k_j_term[0]:k_j_term[1]].columns)]\n",
    "    data_table=DataTable(source=source,columns=columns,width=550,height=300)\n",
    "    show(data_table)\n",
    "    return vector,count\n",
    "\n",
    "# 定义更新corpus选项:\n",
    "def update_corpus_sklearn(new_corpus,corpus_name):#请用列表将所有要添加的corpus(list in list形式)包括。\n",
    "    options=list(corpus_sklearn.options)\n",
    "    for i,name in zip(new_corpus,corpus_name):\n",
    "        options.append((name,i))\n",
    "    corpus_sklearn.options=options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里的corpus中的元素变为用空格分隔的字符串，可以在前面生成用空格分隔的字符串形式，也可以吧列表形式转化为该形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#widget定义\n",
    "corpus_sklearn=widgets.SelectMultiple(description='raw text',options=[],value=(),layout=widgets.Layout(width='auto'))\n",
    "char_corpus=[' '.join(i) for i in corpus]\n",
    "update_corpus_sklearn([char_corpus],['金庸'])\n",
    "corpus_sklearn.value=(char_corpus,)\n",
    "analyzer=widgets.Dropdown(description='analyzer',options=['word','char','charwb'],value='word',layout=widgets.Layout(width='auto'))\n",
    "stop_words=widgets.Text(description='stop_words',layout=widgets.Layout(width='auto'),disabled=True)\n",
    "token_pattern=widgets.Text(description='token_pattern',layout=widgets.Layout(width='auto'),disabled=True)\n",
    "max_df=widgets.FloatSlider(description='max_df',min=0,max=1,step=0.01,value=1,\n",
    "                           layout=widgets.Layout(width='auto'))#从比例的角度限制，没能包括int选项\n",
    "min_df=widgets.FloatText(description='min_df',min=0,max=1,step=0.001,value=0,layout=widgets.Layout(width='auto'))\n",
    "max_features=widgets.IntText(description='max_feature',min=10000,max=1000000,step=1000,value='1000000',\n",
    "                             layout=widgets.Layout(width='auto'))\n",
    "binary=widgets.Checkbox(description='binary',value=False,layout=widgets.Layout(width='auto'))\n",
    "k_j_term=widgets.IntRangeSlider(min=-100,max=100,step=5,value=(0,10),description='显示一定范围的term',\n",
    "                                style={'description_width':'initial'},layout=widgets.Layout(width='400px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sklearn=widgets.interactive_output(sklearn_bow,{'corpus_sklearn':corpus_sklearn,'analyzer':analyzer,'stop_words':stop_words,\n",
    "                                                    'token_pattern':token_pattern,'max_df':max_df,'min_df':min_df,\n",
    "                                                    'max_features':max_features,'binary':binary,'k_j_term':k_j_term})\n",
    "vbox_sklearn=widgets.VBox([corpus_sklearn,analyzer,stop_words,token_pattern,max_df,min_df,max_features,binary],\n",
    "                          layout=widgets.Layout(width='40%'))\n",
    "tab_sklearn=widgets.HBox([vbox_sklearn,widgets.VBox([k_j_term,out_sklearn],layout=widgets.Layout(width='auto'))],\n",
    "                         layout=widgets.Layout(height='350px'))\n",
    "#tab_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为gensim添加更多的语料，请使用该函数。`update_add_documents(new_corpus,corpus_name)`:#请用列表将所有要添加的corpus(list in list形式)包括\n",
    "\n",
    "`update_corpus_sklearn([char_corpus],['金庸'])` 请用这种形式更新sklearn的语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4347455a188a43e5b608fb7c211db553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(VBox(children=(IntSlider(value=10, continuous_update=False, description='显示前k个ter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_vectorize=widgets.Tab([tab_gensim,tab_sklearn])\n",
    "tab_vectorize.set_title(0,'gensim')\n",
    "tab_vectorize.set_title(1,'sklearn')\n",
    "tab_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取gensim生成的bow[doc2bow生成的列表，···]\n",
    "dct_gensim,bow_gensim=gensim_bow(top_k_term=top_k_term.value,add_documents=add_documents.value,no_below=no_below.value,\n",
    "                      no_above=no_above.value,keep_n=keep_n.value,keep_tokens=keep_tokens.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得词频model和词频向量\n",
    "vector_sklearn,count_sklearn=sklearn_bow(corpus_sklearn=corpus_sklearn.value,analyzer=analyzer.value,stop_words=stop_words.value,\n",
    "                                                    token_pattern=token_pattern.value,max_df=max_df.value,min_df=min_df.value,\n",
    "                                                    max_features=max_features.value,binary=binary.value,k_j_term=k_j_term.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf 一个term占一个document总term或最多term的比例\n",
    "\n",
    "idf log（corpus总document数/含有term的document数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba jieba.analyse.extract_tags\n",
    "\n",
    "计算单个document的tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('喜欢', 0.06666666666666667),\n",
       " ('篮球', 0.06666666666666667),\n",
       " ('足球', 0.06666666666666667)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.set_idf_path('data/idf/test.txt')\n",
    "jieba.analyse.extract_tags('喜欢篮球足球大家',withWeight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba.load_userdict()#使用自定义词典\n",
    "#jieba.analyse.set_stop_words()#使用停用词\n",
    "#jieba.analyse.set_idf_path(file_path)#设置计算idf的语料库，而不使用默认的corpus\n",
    "import jieba.analyse as ana\n",
    "out_tfidf_jieba=widgets.Output(layout=widgets.Layout(width='auto',height='auto'))\n",
    "@out_tfidf_jieba.capture(clear_output=True,wait=True)\n",
    "def tfidf_jieba(dict_path,idf_path,stopword_path,topK,sentence,withWeight,allowPOS):\n",
    "    if dict_path:\n",
    "        jieba.load_userdict(dict_path)\n",
    "    if idf_path:\n",
    "        ana.set_idf_path(idf_path)\n",
    "    if stopword_path:\n",
    "        ana.set_stop_words(stop_words_path)\n",
    "    data=ana.extract_tags(sentence=sentence,withWeight=withWeight,allowPOS=allowPOS,topK=topK)\n",
    "    if withWeight:\n",
    "        word= pd.DataFrame(data,columns=['term','tfidf'])\n",
    "        source=ColumnDataSource(word)\n",
    "        columns=[TableColumn(field=i,title=i) for i in list(word.columns)]\n",
    "        data_table=DataTable(source=source,columns=columns,width=400,height=400)   \n",
    "        show(data_table)\n",
    "    else:\n",
    "        word=pd.DataFrame(data,columns=['term'])\n",
    "        source=ColumnDataSource(word)\n",
    "        columns=[TableColumn(field='term',title='term')]\n",
    "        data_table=DataTable(source=source,columns=columns,width=400,height=400)\n",
    "        show(data_table)\n",
    "    return ana.extract_tags(sentence=sentence,#对单个document\n",
    "                            withWeight=withWeight,#tfidf\n",
    "                            allowPOS=allowPOS,#词性\n",
    "                            topK=topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widgets定义\n",
    "dict_path=widgets.Dropdown(options=list(zip(os.listdir('data/词库'),['data/词库/'+i for i in os.listdir('data/词库')]))+[('不设置','')],\n",
    "                           value='',description='自定义字典',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "idf_path=widgets.Dropdown(options=list(zip(os.listdir('data/idf'),['data/idf/'+i for i in os.listdir('data/idf')]))+[('不设置','')],\n",
    "                           value='',description='自定义idf频率',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "stopword_path=widgets.Dropdown(options=list(zip(os.listdir('data/stopword'),['data/stopword/'+i for i in os.listdir('data/stopword')]))+[('不设置','')],\n",
    "                          value='data/stopword/stopwords.txt',description='停用词典',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "topK=widgets.IntText(min=10,max=1000,step=5,value=20,description='前k个term',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "sentence=widgets.Dropdown(options=text_select.options,description='document',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "withWeight=widgets.Checkbox(value=True,description='是否显示权重',layout=widgets.Layout(width='auto'),continuous_update=False)\n",
    "cixing_label=['普通名词','方位名词','处所名词','时间','人名','地名','机构名','作品名','其他专名','普通动词','动副词','名动词','形容词',\n",
    "              '副形词','名形词','副词','数量词','量词','代词','介词','连词','助词','其他虚词','标点符号','人名','地名','机构名','时间']\n",
    "cixing_value=('n','f','s','t','nr','ns','nt','nw','nz','v','vd','vn',\n",
    "             'a','ad','an','d','m','q','r','p','c','u','xc','w','PER','LOC','ORG','TIME')\n",
    "allowPOS=widgets.SelectMultiple(options=list(zip(cixing_label,cixing_value)),description='选择词性',value=cixing_value,\n",
    "                                layout=widgets.Layout(width='auto'),continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tfidf_jieba=widgets.interactive_output(tfidf_jieba,{'dict_path':dict_path,'idf_path':idf_path,'stopword_path':stopword_path,'sentence':sentence,\n",
    "                                          'topK':topK,'withWeight':withWeight,'allowPOS':allowPOS})\n",
    "tab_tfidf_jieba=widgets.HBox([\n",
    "widgets.VBox([dict_path,idf_path,stopword_path,sentence,topK,withWeight,allowPOS],layout=widgets.Layout(width='50%')),\n",
    "widgets.VBox([out_tfidf_jieba])\n",
    "],\n",
    "layout=widgets.Layout(height='450px')\n",
    ")\n",
    "#tab_tfidf_jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回所有文档，也就是语料的tf-idf矩阵，利用向量矩阵也就是词频矩阵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf_sklearn=widgets.Output(layout=widgets.Layout(width='width',height='height'))\n",
    "@out_tfidf_sklearn.capture(clear_ooutput=True,wait=True)\n",
    "def tfidf_sklearn(vector_tfidf_sklearn):\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer#tfi-df\n",
    "    tfidf_model=TfidfTransformer()\n",
    "    if  vector_tfidf_sklearn:\n",
    "        tfidf=tfidf_model.fit_transform(eval(vector_tfidf_sklearn))\n",
    "        #print(tfidf)\n",
    "        return tfidf_model,tfidf\n",
    "def update_vector_sklearn(vector_char):\n",
    "    options=list(vector_tfidf.sklearn.options)\n",
    "    options.extend(vector_char)\n",
    "    vector_tfidf.sklearn.options=options\n",
    "vector_tfidf_sklearn=widgets.Dropdown(options=['vector_sklearn'],value='vector_sklearn',description='词频矩阵')\n",
    "tfidf_sklearn_code=widgets.HTML(\n",
    "    value=\n",
    "    '''\n",
    "    <code>    \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer#\n",
    "    transformer=TfidfTransformer()\n",
    "    tfidf=transformer.fit_transform(vector_sklearn)\n",
    "    </code>\n",
    "    ''',layout=widgets.Layout(width='50%'))\n",
    "with out_tfidf_sklearn:\n",
    "    w_tfidf_sklearn=widgets.interact_manual(tfidf_sklearn,vector_tfidf_sklearn=vector_tfidf_sklearn)\n",
    "tab_tfidf_sklearn=widgets.HBox(\n",
    "[\n",
    "tfidf_sklearn_code,\n",
    "out_tfidf_sklearn\n",
    "],layout=widgets.Layout(height='200px'))\n",
    "#tab_tfidf_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interact_manual可以输出返回值，而interactive_output不可以,但是所有的部件不能被布局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_vector_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接利用前面的corpus，因为中间过程中还是需要用到corpus，所以不接着用前面的gensim结果。所以这里我们可以提供前面的多个corpus结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf_gensim=widgets.Output(layout=widgets.Layout(height='10000px'))\n",
    "@out_tfidf_gensim.capture(clear_output=True,wait=True)\n",
    "def tfidf_gensim(tfidf_gensim_vector):\n",
    "    from gensim import models\n",
    "    tfidf_model=models.TfidfModel(eval(tfidf_gensim_vector))#建立TF-idf模型\n",
    "    tfidf_gensim_result=tfidf_model[eval(tfidf_gensim_vector)]#获取tfidf模型结果\n",
    "    #也可以只传入某一个document的doc2bow，这样只会获得该document的tfidf\n",
    "    print(tfidf_model)\n",
    "    print(tfidf_gensim_result)\n",
    "    return tfidf_model,tfidf_gensim_result#list in list [[(id,tfidf)],[]]\n",
    "tfidf_gensim_vector=widgets.Dropdown(options=['bow_gensim'],value='bow_gensim',description='分好词的corpus')\n",
    "def update_tfidf_gensim_vector(char_bow):#列表形式\n",
    "    a=list(tfidf_gensim_vector.options)\n",
    "    [a.append(i) for i in char_bow]\n",
    "    tfidf_gensim_vector.options=a\n",
    "w_tfidf_gensim=widgets.interactive_output(tfidf_gensim,{'tfidf_gensim_vector':tfidf_gensim_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_tfidf_gensim=widgets.HBox(\n",
    "[widgets.HTML(\n",
    "\"\"\"\n",
    "<code>\n",
    "from gensim import corpora,models\n",
    "#生成文档对应的字典和bow2稀疏向量\n",
    "dictionary=corpora.Dictionary(corpus)\n",
    "corpus_tfidfmodel=[dictionary.doc2bow(text) for text in corpus]\n",
    "tfidf_model=models.TfidfModel(corpus_tfidfmodel)#建立TF-idf模型\n",
    "corpus_tfidf=tfidf_model[corpus_tfidfmodel]#获取tfidf模型结果\n",
    "corpus_tfidf#list in list [[(id,tfidf)],[]]\n",
    "corpus_tfidf[3]\n",
    "</code>\n",
    "\"\"\"),\n",
    "widgets.VBox([tfidf_gensim_vector,out_tfidf_gensim])\n",
    "],layout=widgets.Layout(height='300px'))\n",
    "#tab_tfidf_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba只能计算单个document的tfidf，而且idf的计算是依据自定义语料。\n",
    "\n",
    "使用update_vector_sklearn([vector的字符串形式，])来更新vector的可选项,例如['vector1','vector2']\n",
    "\n",
    "使用update_tfidf_gensim_vector([vector对象的字符串形式])来添加gensin可选的语料,例如['bow_gensim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449cadf27934473c89abf717bc5c4a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(VBox(children=(Dropdown(description='自定义字典', index=6, layout=Layout(width='auto')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_tfidf=widgets.Tab([tab_tfidf_jieba,tab_tfidf_sklearn,tab_tfidf_gensim])\n",
    "tab_tfidf.set_title(0,'jieba')\n",
    "tab_tfidf.set_title(1,'sklearn')\n",
    "tab_tfidf.set_title(2,'gensim')\n",
    "tab_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba_tfidf=tfidf_jieba(dict_path=dict_path.value,idf_path=idf_path.value,stopword_path=stopword_path.value,topK=topK.value,\n",
    "                        sentence=sentence.value,withWeight=withWeight.value,allowPOS=allowPOS.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_tfidf_model,sklearn_tfidf=tfidf_sklearn(vector_tfidf_sklearn=vector_tfidf_sklearn.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_tfidf_model,gensim_tfidf=tfidf_gensim(tfidf_gensim_vector=tfidf_gensim_vector.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lda主题模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个document有很多个主题构成，选择合适的主题topic，一个主题里面又包括很多依概率分布的词term。\n",
    "\n",
    "一般是构建三层贝叶斯模型\n",
    "\n",
    "![](data/主题模型.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用词频矩阵\n",
    "\n",
    "也可以计算tfidf，然后传入\n",
    "\n",
    "计算的是整个语料的主题以及对应的term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #主题打印函数\n",
    "    def print_top_words(model,feature_names,n_top_words):\n",
    "        for topic_idx,topic in enumerate(model.components_):\n",
    "            print('topic #%d:'% topic_idx)\n",
    "            print(' '.join(feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]))\n",
    "            print()\n",
    "    n_top_words=12\n",
    "    tf_feature_names=countvec.get_feature_names()\n",
    "    print_top_words(ldamodel,tf_feature_names,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "#pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "out_lda_sklearn=widgets.Output(layout=widgets.Layout(width='auto'))\n",
    "#@out_lda_sklearn.capture(clear_output=True,wait=True)\n",
    "def lda_sklearn(lda_sklearn_model,lda_sklearn_vt,n_components,doc_topic_prior,topic_word_prior,learning_method,learning_decay,learning_offset,total_samples,batch_size):\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    if learning_method=='online':        \n",
    "        ldamodel=LatentDirichletAllocation(n_components=n_components,\n",
    "                                           doc_topic_prior=doc_topic_prior if doc_topic_prior else None,\n",
    "                                           topic_word_prior=topic_word_prior if topic_word_prior else None,\n",
    "                                           learning_method=learning_method,learning_decay=learning_decay,learning_offset=learning_offset,\n",
    "                                          total_samples=total_samples,batch_size=batch_size)\n",
    "    else:\n",
    "        ldamodel=LatentDirichletAllocation(n_components=n_components,doc_topic_prior=doc_topic_prior,topic_word_prior=topic_word_prior,\n",
    "                                           learning_method=learning_method)\n",
    "    result=ldamodel.fit(eval(lda_sklearn_vt))\n",
    "    #print('ldamodel.components_:/n{}':ldamodel.components_)\n",
    "    #print('这是数组形式，每一行代表一个主题，以及主题中各个词的概率\\n要想获得词，需要借助count的get_feature_names')\n",
    "    return ldamodel,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#widgets定义\n",
    "lda_sklearn_model=widgets.Dropdown(options=['count_sklearn'],value='count_sklearn',\n",
    "                                   description='count',layout=widgets.Layout(width='30%'))\n",
    "lda_sklearn_vt=widgets.Dropdown(options=['sklearn_tfidf','vector_sklearn'],value='sklearn_tfidf',\n",
    "                                   description='vector或tfidf',layout=widgets.Layout(width='30%'))\n",
    "n_components=widgets.IntText(value=10,step=1,continuous_update=False,\n",
    "                            description='主题数',layout=widgets.Layout(width='30%'))\n",
    "doc_topic_prior=widgets.FloatText(value=0,step=0.001,style = {'description_width': 'initial'},continuous_update=False,\n",
    "                                 description='文档主题先验参数α',layout=widgets.Layout(width='30%'))\n",
    "topic_word_prior=widgets.FloatText(value=0,step=0.001,style = {'description_width': 'initial'},continuous_update=False,\n",
    "                                 description='主题词先验参数',layout=widgets.Layout(width='30%'))\n",
    "prior_control=widgets.Checkbox(value=True,description='控制先验参数',layout=widgets.Layout(width='30%'))\n",
    "learning_method=widgets.Dropdown(options=['online','batch'],value='online',style = {'description_width': 'initial'},\n",
    "                                description='学习方法',layout=widgets.Layout(width='50%'))\n",
    "learning_decay=widgets.FloatSlider(min=0.5,max=1,value=0.7,step=0.01,style = {'description_width': 'initial'},\n",
    "                                  description='learning_decay',layout=widgets.Layout(width='50%'),continuous_update=False)\n",
    "learning_offset=widgets.IntText(value=10,step=1,style = {'description_width': 'initial'},\n",
    "                               description='learning_offset',layout=widgets.Layout(width='50%'),continuous_update=False)\n",
    "total_samples=widgets.IntText(value=100000,style = {'description_width': 'initial'},\n",
    "                             description='total_samples',layout=widgets.Layout(width='50%'),continuous_update=False)\n",
    "batch_size=widgets.IntText(value=128,style = {'description_width': 'initial'},\n",
    "                          description='batch_size',layout=widgets.Layout(width='50%'),continuous_update=False)\n",
    "button_lda_sklearn=widgets.Button(description='点击生成图像',style={'button_color':'lightgreen'},\n",
    "                                  layout=widgets.Layout(width='50%'))\n",
    "button_clear_output=widgets.Button(description='清空图像',style={'button_color':'lightgreen'},\n",
    "                                   layout=widgets.Layout(width='50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Link(source=(Checkbox(value=True, description='控制先验参数', layout=Layout(width='30%')), 'value'), target=(FloatTe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Link(source=(Checkbox(value=True, description='控制先验参数', layout=Layout(width='30%')), 'value'), target=(FloatTe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# widgets联系\n",
    "\n",
    "widgets.jslink((prior_control,'value'),(doc_topic_prior,'disabled'))\n",
    "widgets.jslink((prior_control,'value'),(topic_word_prior,'disabled'))\n",
    "def update_method(change):\n",
    "    if change['new']=='batch':\n",
    "        learning_decay.disabled=True\n",
    "        learning_offset.disabled=True\n",
    "        total_samples.disabled=True\n",
    "        batch_size.disabled=True\n",
    "    else:\n",
    "        learning_decay.disabled=False\n",
    "        learning_offset.disabled=False\n",
    "        total_samples.disabled=False\n",
    "        batch_size.disabled=False   \n",
    "learning_method.observe(update_method,'value')\n",
    "\n",
    "def update_lda_sklearn_model(char_model):\n",
    "    a=list(lda_sklearn_model.options)\n",
    "    a.extend(char_model)\n",
    "    lda_sklearn_model.options=a\n",
    "    \n",
    "def update_lda_sklearn_vt(char_vt):\n",
    "    a=list(lda_sklearn_tv.options)\n",
    "    a.extend(char_model)\n",
    "    lda_sklearn_model.options=a   \n",
    "\n",
    "def lda_visualization(b):\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.sklearn\n",
    "    pyLDAvis.enable_notebook()\n",
    "    out_lda_sklearn.append_display_data(pyLDAvis.sklearn.prepare(\n",
    "        lda_sklearn(lda_sklearn_vt=lda_sklearn_vt.value,lda_sklearn_model=lda_sklearn_model.value,n_components=n_components.value,\n",
    "                    doc_topic_prior=doc_topic_prior.value,topic_word_prior=topic_word_prior.value,learning_method=learning_method.value,\n",
    "                    learning_decay=learning_decay.value,learning_offset=learning_offset.value,total_samples=total_samples.value,\n",
    "                    batch_size=batch_size.value)[0],\n",
    "        eval(lda_sklearn_vt.value),\n",
    "        eval(lda_sklearn_model.value)))\n",
    "button_lda_sklearn.on_click(lda_visualization)\n",
    "\n",
    "def lda_clear_output(b):\n",
    "    out_lda_sklearn.clear_output()\n",
    "button_clear_output.on_click(lda_clear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 布局\n",
    "image_canshu=widgets.Image(value='data/参数截图/sklearn_lda_参数.png'.encode('utf-8'),format='url',width='auto',height='auto')\n",
    "w_lda_sklearn=widgets.interactive_output(lda_sklearn,{'lda_sklearn_vt':lda_sklearn_vt,'lda_sklearn_model':lda_sklearn_model,\n",
    "                                                     'n_components':n_components,'doc_topic_prior':doc_topic_prior,\n",
    "                                                      'topic_word_prior':topic_word_prior,'learning_method':learning_method,\n",
    "                                                     'learning_decay':learning_decay,'learning_offset':learning_offset,\n",
    "                                                     'total_samples':total_samples,'batch_size':batch_size})\n",
    "tab_lda_sklearn=widgets.Tab(\n",
    "[\n",
    "widgets.VBox(\n",
    "[\n",
    "    widgets.HBox([lda_sklearn_model,lda_sklearn_vt,n_components],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([prior_control,doc_topic_prior,topic_word_prior],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([learning_method],layout=widgets.Layout(width='50%')),\n",
    "    widgets.HBox([learning_decay,learning_offset],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([total_samples,batch_size],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([button_lda_sklearn,button_clear_output],layout=widgets.Layout(width='100%')),\n",
    "    out_lda_sklearn\n",
    "],layout=widgets.Layout(width='auto')),\n",
    "image_canshu\n",
    "],\n",
    "layout=widgets.Layout(height='1000px',width='100%')\n",
    ")\n",
    "tab_lda_sklearn.set_title(0,'lda_sklearn')\n",
    "tab_lda_sklearn.set_title(1,'参数')\n",
    "#tab_lda_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def lda_gensim(lda_gensim_dct,lda_gensim_tfidf,num_topics,passes):\n",
    "    from gensim import corpora,models\n",
    "    #生成文档对应的字典和bow2稀疏向量\n",
    "    dictionary=corpora.Dictionary(corpus)\n",
    "    corpus_bow_gensim=[dictionary.doc2bow(text) for text in corpus]\n",
    "    tfidf_model=models.TfidfModel(corpus_bow_gensim)#建立TF-idf模型\n",
    "    corpus_tfidf_gensim=tfidf_model[corpus_bow_gensim]#获取tfidf模型结果\n",
    "    from gensim.models.ldamodel import LdaModel\n",
    "    ldamodel=LdaModel(corpus_tfidf_gensim,id2word=dictionary,num_topics=num_topics,passes=passes)#建立lda模型，获得lda模型结果\n",
    "    result_lda_gensim=ldamodel[corpus_tfidf_gensim]#每个文档的各主题的概率分布\n",
    "    #ldamodel.print_topics()查看各个主题的term分布\n",
    "    return ldamodel,result_lda_gensim\n",
    "  \n",
    "def update_corpus_lda_gensim(new_corpus,corpus_name):#请用列表将所有要添加的corpus(list in list形式)包括。\n",
    "    options=list(corpus_lda_gensim.options)\n",
    "    for i,name in zip(new_corpus,corpus_name):\n",
    "        options.append((name,i))\n",
    "    corpus_lda_gensim.options=options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gensim(lda_gensim_dct,lda_gensim_vt,num_topics,passes):\n",
    "    from gensim.models.ldamodel import LdaModel\n",
    "    ldamodel=LdaModel(eval(lda_gensim_vt),id2word=eval(lda_gensim_dct),num_topics=num_topics,passes=passes)#建立lda模型，获得lda模型结果\n",
    "    result_lda_gensim=ldamodel[eval(lda_gensim_vt)]#每个文档的各主题的概率分布\n",
    "    #ldamodel.print_topics()查看各个主题的term分布\n",
    "    return ldamodel,result_lda_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gensim_dct=widgets.Dropdown(description='dct',options=['dct_gensim'],value='dct_gensim',layout=widgets.Layout(width='50%'))\n",
    "lda_gensim_vt=widgets.Dropdown(description='bow或tfidf',options=['bow_gensim','gensim_tfidf'],value='bow_gensim',layout=widgets.Layout(width='50%'))\n",
    "\n",
    "passes=widgets.IntSlider(value=1,step=1,description='重复次数',continuous_update=False,layout=widgets.Layout(width='50%'))\n",
    "num_topics=widgets.IntText(value=10,step=1,\n",
    "                            description='主题数',layout=widgets.Layout(width='50%'),continuous_update=False)\n",
    "out_lda_gensim=widgets.Output(layout=widgets.Layout(width='auto'))\n",
    "\n",
    "button_lda_gensim=widgets.Button(description='点击生成图像',style={'button_color':'lightgreen'},\n",
    "                                  layout=widgets.Layout(width='50%'),disabled=False)\n",
    "button_gensim_clear_output=widgets.Button(description='清空图像',style={'button_color':'lightgreen'},\n",
    "                                  layout=widgets.Layout(width='50%'),disabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义更新dct,tfidf选项:\n",
    "def update_lda_gensim_dct(char_dct):\n",
    "    a=list(lda_gensim_dct.options)\n",
    "    a.extend(char_dct)\n",
    "    lda_gensim_dct.options=a\n",
    "    \n",
    "def update_lda_sklearn_vt(char_vt):\n",
    "    a=list(lda_gensim_vt.options)\n",
    "    a.extend(char_vt)\n",
    "    lda_gensim_vt.options=a \n",
    "    \n",
    "def lda_gensim_visualization(b):\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim_models\n",
    "    pyLDAvis.enable_notebook()\n",
    "    a=lda_gensim(lda_gensim_dct=lda_gensim_dct.value,lda_gensim_vt=lda_gensim_vt.value,passes=passes.value,num_topics=num_topics.value)\n",
    "    out_lda_gensim.append_display_data(pyLDAvis.gensim_models.prepare(a[0],eval(lda_gensim_vt.value),eval(lda_gensim_dct.value)))\n",
    "button_lda_gensim.on_click(lda_gensim_visualization)\n",
    "\n",
    "def lda_gensim_clear_output(b):\n",
    "    out_lda_gensim.clear_output()\n",
    "button_gensim_clear_output.on_click(lda_gensim_clear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_lda_gensim=widgets.VBox(\n",
    "[\n",
    "    widgets.HBox([lda_gensim_dct,lda_gensim_vt],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([passes,num_topics],layout=widgets.Layout(width='100%')),\n",
    "    widgets.HBox([button_lda_gensim,button_gensim_clear_output],layout=widgets.Layout(width='100%')),\n",
    "    out_lda_gensim\n",
    "],layout=widgets.Layout(height='800px',width='100%'))\n",
    "\n",
    "#tab_lda_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用update_lda_sklearn_model(model对象字符串形式的列表),比如['count','tfidfmodel']\n",
    "\n",
    "使用update_lda_sklearn_vt(vector或tfidf对象的字符串形式的列表),比如['vector_sklearn'.'sklearn_tfidf']\n",
    "\n",
    "使用update_lda_gensim_dct(char_dct)更新gensim的字典或tfidf模型\n",
    "\n",
    "使用update_lda_gensim_vt(char_vt)更新gensim的bow或tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d7234ab4864bcca96533cba7b262a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Tab(children=(VBox(children=(HBox(children=(Dropdown(description='vector', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_lda=widgets.Tab([tab_lda_sklearn,tab_lda_gensim])\n",
    "tab_lda.set_title(0,'sklearn')\n",
    "tab_lda.set_title(1,'gensim')\n",
    "tab_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_lda_model,sklearn_lda=lda_sklearn(lda_sklearn_model=lda_sklearn_model.value,lda_sklearn_vt=lda_sklearn_vt.value,\n",
    "                                          n_components=n_components.value,doc_topic_prior=doc_topic_prior.value,\n",
    "                                          topic_word_prior=topic_word_prior.value,learning_method=learning_method.value,\n",
    "                                          learning_decay=learning_decay.value,learning_offset=learning_offset.value,\n",
    "                                          total_samples=total_samples.value,batch_size=batch_size.value)\n",
    "gensim_lda_model,gensim_lda=lda_gensim(lda_gensim_dct=lda_gensim_dct.value,lda_gensim_vt=lda_gensim_vt.value,\n",
    "                                       num_topics=num_topics.value,passes=passes.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 使用词频计算，每个document都有对于所有term的词频信息，依次计算余弦相似度\n",
    " \n",
    " 对每一个document提取相同数量的关键词，然后组合在一起，包含位置信息，填充tfidf值，计算余弦相似度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<41x46212 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 133756 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.97808028, 0.97619018, ..., 0.98382802, 0.96162715,\n",
       "        0.91063748],\n",
       "       [0.97808028, 0.        , 0.64394169, ..., 0.84081498, 0.81197049,\n",
       "        0.85385464],\n",
       "       [0.97619018, 0.64394169, 0.        , ..., 0.84790029, 0.69627883,\n",
       "        0.85854843],\n",
       "       ...,\n",
       "       [0.98382802, 0.84081498, 0.84790029, ..., 0.        , 0.35787095,\n",
       "        0.47134396],\n",
       "       [0.96162715, 0.81197049, 0.69627883, ..., 0.35787095, 0.        ,\n",
       "        0.57492349],\n",
       "       [0.91063748, 0.85385464, 0.85854843, ..., 0.47134396, 0.57492349,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "#pairwise_distances(词频矩阵或一部分tfidf关键词)\n",
    "pairwise_distances(vector_sklearn,metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.99197224, 0.99090365, 0.99103797, 0.99327461],\n",
       "       [0.99197224, 0.        , 0.72229363, 0.89455975, 0.9108459 ],\n",
       "       [0.99090365, 0.72229363, 0.        , 0.77775368, 0.72896289],\n",
       "       [0.99103797, 0.89455975, 0.77775368, 0.        , 0.601107  ],\n",
       "       [0.99327461, 0.9108459 , 0.72896289, 0.601107  , 0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_distances(sklearn_tfidf[:5],metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/参数截图/sklearn_文档相似度1.jpg)\n",
    "![](data/参数截图/sklearn_文档相似度2.jpg)\n",
    "![](data/参数截图/sklearn_文档相似度3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d0aea1c70c412cbbbf5ba2692883da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(SelectMultiple(description='计算哪些文档的相似度', layout=Layout(width='auto'), options=(1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#函数定义\n",
    "out_similarity_sklearn=widgets.Output()\n",
    "@out_similarity_sklearn.capture(clear_output=True,wait=True)\n",
    "def similarity_sklearn(similarity_sklearn_vt,documents,metric):\n",
    "    from sklearn.metrics.pairwise import pairwise_distances\n",
    "    data=eval(similarity_sklearn_vt)\n",
    "    if documents:\n",
    "        import scipy.sparse as sp\n",
    "        data=sp.vstack((data[i-1] for i in documents),format='csr')    \n",
    "    print(pairwise_distances(data,metric=metric))\n",
    "    return pairwise_distances(data,metric=metric)\n",
    "\n",
    "#widgets定义\n",
    "\n",
    "select_documents=widgets.SelectMultiple(options=list(range(1,len(text_select.options))),value=(),description='计算哪些文档的相似度',\n",
    "                                        style={'description_width':'initial'},layout=widgets.Layout(width='auto'),\n",
    "                                       continuous_update=False)\n",
    "similarity_sklearn_vt=widgets.Dropdown(options=['vector_sklearn','sklearn_tfidf'],value='sklearn_tfidf',description='vector或tfidf',\n",
    "                                       style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "metric=widgets.Dropdown(options=['cityblock','cosine','euclidean','l1','l2','manhatten'],value='cosine',description='空间距离计算方式',\n",
    "                        style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "\n",
    "# 定义布局\n",
    "w_similarity_sklearn=widgets.interactive_output(similarity_sklearn,\n",
    "                                                {'documents':select_documents,\n",
    "                                                 'similarity_sklearn_vt':similarity_sklearn_vt,\n",
    "                                                 'metric':metric})\n",
    "tab_similarity_sklearn=widgets.HBox(\n",
    "[\n",
    "    widgets.VBox([select_documents,similarity_sklearn_vt,metric],layout=widgets.Layout(width='40%')),\n",
    "    out_similarity_sklearn\n",
    "],\n",
    "layout=widgets.Layout(width='auto',height='auto')\n",
    ")\n",
    "tab_similarity_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from gensim.models.word2vec import Word2Vec\n",
    "    w2vmodel=gensim.models.word2vec.Word2Vec()#模型对象\n",
    "    w2vmodel.build_vocab(corpus)#词典生成\n",
    "    w2vmodel.train(corpus)#训练模型\n",
    "    w2vmodel.wv['term']\n",
    "    w2vmodel.wv['term'].shape()\n",
    "    #保存模型和导入模型\n",
    "    w2vmodel.save(存盘路径和文件名称)\n",
    "    w2vmodel.load(文件路径)\n",
    "    w2vmodel.wv.most_similar(term,topn)\n",
    "    #寻找对应关系\n",
    "    w2vmodel.wv.most_similar([],[],topn)\n",
    "    #计算两个词的相似度和相关度\n",
    "    w2vmodel.wv.similarity('term','term')\n",
    "    #寻找不合群的词\n",
    "    w2vmodel.wv.doesnt_match('term1 term2 term3'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/参数截图/gensim_文档相似度.jpg)\n",
    "![](data/参数截图/gensim_文档相似度_train.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数定义\n",
    "out_similarity_gensim=widgets.Output(layout=widgets.Layout(width='550px',height='auto'))\n",
    "@out_similarity_gensim.capture(clear_output=True,wait=True)\n",
    "def similarity_gensim(sentences,vector_size,window,workers,min_count,max_vocab_size,topn,select_term1,select_term2):\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    w2vmodel=Word2Vec(sentences=sentences,vector_size=vector_size,window=window,workers=workers,\n",
    "                      min_count=min_count,max_vocab_size=max_vocab_size)\n",
    "    w2vmodel.build_vocab(sentences)  # prepare the model vocabulary=\n",
    "    w2vmodel.train(sentences,total_examples=w2vmodel.corpus_count,epochs=w2vmodel.epochs)\n",
    "    word_vectors=w2vmodel.wv\n",
    "    if select_term1 and not select_term2:\n",
    "        print('{0}的前{1}个最相似的term是:'.format(select_term1.split(),topn))\n",
    "        print(word_vectors.most_similar(select_term1.split(),topn=topn))\n",
    "        if len(select_term1.split())>=3:\n",
    "            print()\n",
    "            print()\n",
    "            print('{0}中最不合群的词是:{1}'.format(select_term1.split(),word_vectors.doesnt_match(select_term1.split())))\n",
    "    if select_term1 and select_term2:\n",
    "        print('{0}和{1}的对应最相似的前{2}个term是:'.format(select_term1.split(),select_term2.split(),topn))\n",
    "        print(word_vectors.most_similar(select_term1.split(),select_term2.split(),topn=topn))\n",
    "        print()\n",
    "        print()\n",
    "        for i in select_term1.split():\n",
    "            for j in select_term2.split():\n",
    "                print('{0}和{1}的相似性为{2}'.format(i,j,word_vectors.similarity(i,j)))\n",
    "    return w2vmodel,word_vectors\n",
    "\n",
    "# widgets定义\n",
    "sentences=widgets.Dropdown(options=[('corpus_name',corpus)],value=corpus,description='语料',\n",
    "                          style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "vector_size=widgets.IntText(value=100,step=10,continuous_update=False,description='数据维度',description_tooltip='vector_size',\n",
    "                           style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "window=widgets.IntSlider(min=1,max=10,value=5,step=1,continuous_update=False,description='上下文个数',description_tooltip='window',\n",
    "                        style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "workers=widgets.IntSlider(min=1,max=3,value=3,step=1,disabled=True,\n",
    "                          style={'description_width':'initial'},layout=widgets.Layout(width='auto'),\n",
    "                          continuous_update=False,description='同时运行的线程数',description_tooltip='workers')\n",
    "min_count=widgets.IntText(value=1,step=1,continuous_update=False,description='最少含有term的文档数',description_tooltip='min_count',\n",
    "                         style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "max_vocab_size=widgets.IntText(value=1000000,continuous_update=False,description='最多词条数',description_tooltip='max_vocab_size',\n",
    "                              style={'description_width':'initial'},layout=widgets.Layout(width='auto'))\n",
    "topn=widgets.IntText(value=10,continuous_update=False,description='前k个最相似的term',description_tooltip='topn',\n",
    "                    style={'description_width':'initial'},layout=widgets.Layout(width='400px'))\n",
    "select_term1=widgets.Text(value='郭靖',continuous_update=False,description='positive term',description_tooltip='输入空格分隔的字符串',\n",
    "                         style={'description_width':'initial'},layout=widgets.Layout(width='500px'))\n",
    "select_term2=widgets.Text(value='',continuous_update=False,description='negative term',description_tooltip='输入空格分隔的字符串',\n",
    "                         style={'description_width':'initial'},layout=widgets.Layout(width='500px'))\n",
    "\n",
    "# widgets的call\n",
    "\n",
    "def upate_similarity_gensim_sentences(corpus_name,corpus):\n",
    "    a=list(sentences.options)\n",
    "    [a.append(i) for i in zip(corpus_name,corpus)]\n",
    "    sentences.options=a\n",
    "\n",
    "# 定义布局\n",
    "\n",
    "w_similarity_gensim=widgets.interactive_output(similarity_gensim,{'sentences':sentences,'vector_size':vector_size,'window':window,\n",
    "                                                                 'workers':workers,'min_count':min_count,'topn':topn,\n",
    "                                                                 'max_vocab_size':max_vocab_size,'select_term1':select_term1,\n",
    "                                                                 'select_term2':select_term2})\n",
    "tab_similarity_gensim=widgets.HBox(\n",
    "[\n",
    "  widgets.VBox([sentences,vector_size,window,workers,min_count,max_vocab_size],layout=widgets.Layout(width='40%',height='auto')),\n",
    "  widgets.VBox([select_term1,select_term2,topn,out_similarity_gensim],layout=widgets.Layout(width='auto'))\n",
    "]\n",
    ")\n",
    "#tab_similarity_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151174, 1535275)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x2692eb47188>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel=Word2Vec(sentences=corpus,vector_size=100)\n",
    "w2vmodel.build_vocab(corpus)  # prepare the model vocabulary\n",
    "w2vmodel.train(corpus,total_examples=w2vmodel.corpus_count,epochs=w2vmodel.epochs)\n",
    "w2vmodel.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56e9757341749f29cd4151f1be39010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HBox(children=(VBox(children=(SelectMultiple(description='计算哪些文档的相似度', index=(2, 3, 4), layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_similarity=widgets.Tab([tab_similarity_sklearn,tab_similarity_gensim])\n",
    "tab_similarity.set_title(0,'sklearn')\n",
    "tab_similarity.set_title(1,'gensim')\n",
    "tab_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_similarity=similarity_sklearn(similarity_sklearn_vt=similarity_sklearn_vt.value,documents=select_documents.value,\n",
    "                                      metric=metric.value)\n",
    "gensim_w2vmodel,gensim_similarity=similarity_gensim(sentences=sentences.value,vector_size=vector_size.value,window=window.value,\n",
    "                                                    workers=workers.value,min_count=min_count.value,max_vocab_size=max_vocab_size.value,\n",
    "                                                    topn=topn.value,select_term1=select_term1.value,select_term2=select_term2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.63876711, 0.63599507],\n",
       "       [0.63876711, 0.        , 0.52892109],\n",
       "       [0.63599507, 0.52892109, 0.        ]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2692c847e88>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_w2vmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05545413,  0.5324912 ,  1.9462761 ,  0.8149726 , -0.15901993,\n",
       "       -1.3808631 ,  0.47726837,  1.3543433 , -0.19652618, -1.0946499 ,\n",
       "       -0.3410111 , -1.953739  ,  0.10910839,  0.9589614 , -0.06241031,\n",
       "       -0.01312604,  0.52641034, -1.0334115 , -0.0487875 , -2.8216789 ,\n",
       "        1.4789106 ,  0.05305295,  0.84040594, -0.7954556 , -0.6782701 ,\n",
       "        0.10408469, -0.13413447, -0.08345853, -1.1278182 ,  0.8210653 ,\n",
       "        0.67609453,  0.20185457,  0.49350926, -1.6392524 , -0.27027863,\n",
       "        0.29041296, -0.0497606 , -0.9042903 , -1.2524049 , -1.4134364 ,\n",
       "        0.70933735, -1.0513176 , -0.92195874,  0.19093885,  0.5690779 ,\n",
       "       -1.2498894 , -1.1133691 , -0.21581416, -0.04807532,  1.2843019 ,\n",
       "        0.45296296, -1.4255974 , -0.01906197, -1.3504627 , -0.9461467 ,\n",
       "        0.11410528,  0.05026502,  0.31246385, -0.88261956,  0.16633731,\n",
       "        0.55407506, -0.1227994 , -0.90907556,  0.45469612, -0.19736755,\n",
       "        0.5850681 ,  0.51382166,  1.1307993 , -1.2444685 ,  0.7198156 ,\n",
       "       -0.06455763,  0.0656859 ,  1.7148765 , -1.1795039 ,  1.0379565 ,\n",
       "       -0.94112074,  0.45441103, -0.31454185, -0.9280597 ,  0.08657511,\n",
       "       -0.88529205, -0.05945396, -0.45525452,  1.2916211 , -0.55156946,\n",
       "       -0.24106358,  0.76767904,  1.1936464 ,  0.2915935 ,  0.5561054 ,\n",
       "        1.5456175 , -0.80895585, -0.16658823,  1.0994008 ,  2.0664055 ,\n",
       "        1.3759671 ,  0.30406684, -0.68037343,  0.8025622 ,  0.19683029],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_similarity['郭靖']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于词频矩阵或tfidf或lda主题来进行文本分类\n",
    "\n",
    "原则上所有分类的算法都能应用于文本分类\n",
    "\n",
    "文本分类具有的特点：维度极高，自变量相关性问题，自变量有效性问题\n",
    "\n",
    "算法简单的贝叶斯算法往往会成为优先考虑的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用d2m矩阵"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
